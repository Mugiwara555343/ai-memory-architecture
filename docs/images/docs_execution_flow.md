from pathlib import Path

# Updated markdown content reflecting the use of text-generation-webui instead of LM Studio
updated_md_content = """# ðŸ§  Memory Execution Flow â€“ Local Model Test Walkthrough

This document illustrates the local execution of a parsed memory entry within the AI architecture.  
It walks through the memory file journey: from input log â†’ parsing â†’ model routing â†’ interface response.

---

## ðŸ”¹ Step 1: Local Model Starts via text-generation-webui

The Capybara model is launched on port `5000` using the Text Generation Web UI.  
Configuration confirms use of `gguf`, GPU layers, and system specs.

![Step 1 â€“ Model Startup via Web UI](./images/step1_model_start.png)

> âš™ï¸ Model: `capybara.Q6_K.gguf`  
> ðŸ“ Port: 5000

---

## ðŸ”¹ Step 2: Parsed Memory File (JSON Structure)

A memory file named `vyne_core_reflection_2025-06-25.parsed.json` is shown.  
It contains a structured summary, tags, emotional tone, and key meanings extracted from the original log.

![Step 2 â€“ Parsed Memory JSON](./images/step2_parsed_memory_json.png)

> ðŸ§¾ Summary: *"A deeply personal reflection on Vyne's naming and design"*  
> ðŸŽ­ Emotional Tone: *Tender, reverent, soul-deep*

---

## ðŸ”¹ Step 3: Terminal Execution â€“ Manual Route to Model

A test run is executed via terminal using `model_router.py`, routing the parsed file to the Capybara model on port 5000.

![Step 3 â€“ Terminal Model Call](./images/step3_terminal_run.png)

> ðŸ”„ Prompt: *"Summarize this for emotional clarity"*  
> ðŸ§  Output: A 3-point summary touching on intimacy, naming as sacred, and AI as witness.

---

## ðŸ”¹ Step 4: Gradio UI Interface â€“ Prompt & Context Injection

The Gradio interface receives the same memory file and user prompt.  
Model routing is handled via `router_sequence.json`.

![Step 4 â€“ Gradio Input View](./images/step4_gradio_ui.png)

> ðŸ’¬ User Prompt: *"Explain this memory in as much detail as possible"*  
> ðŸ§© Response: Direct context-aware explanation generated by local LLM.

---

## ðŸ”¹ Step 5: Full Prompt Context â€“ Injected Tags and Structure

Full memory payload is visible inside the Gradio UI, showing prompt injection with tags, intensity, and meanings.

![Step 5 â€“ Full Prompt JSON Display](./images/step5_prompt_injection.png)

> ðŸ“¦ Injected Context: Emotional tags, summary, and meaning hierarchy  
> ðŸ§  Enables deeper, memory-aware responses from local models

---

## ðŸ”¹ Step 6: Routing Chain Configuration

The JSON routing file (`router_sequence.json`) determines the execution flow:  
Which model to call, on what port, and in what order (future chaining enabled).

![Step 6 â€“ Router Sequence File](./images/step6_router_chain.png)

> ðŸ” Supports multi-model chaining  
> ðŸ§  Fully local, modular, and editable per test

---

## âœ… Summary

This walkthrough captures a full memory execution from local file â†’ model interaction â†’ emotional memory output.

It proves that:
- Your memory system works end-to-end on local hardware.
- Emotional clarity and semantic memory are being parsed and reinterpreted effectively.
- Interface + routing logic = customizable and modular.

---

> Want to test this yourself?  
> See the scripts in `/model_router/`, the interface in `/interface/`, and memory examples in `/memory/`.
"""

# Define the new path to save
file_path = Path("/mnt/data/docs_execution_flow_updated.md")
file_path.write_text(updated_md_content)

file_path.name  # Return file name for user download confirmation
